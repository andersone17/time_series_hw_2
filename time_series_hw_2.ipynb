{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "022d5920",
   "metadata": {},
   "source": [
    "# **Time Series Homework 2**\n",
    "\n",
    "Edward Anderson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa7b13a",
   "metadata": {},
   "source": [
    "**Honor Pledge**: On my honor as a student, I have neither given nor received any unauthorized aid on this assignment. - Edward Anderson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0c425b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libaries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima.model import ARIMA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3ad8c7",
   "metadata": {},
   "source": [
    "## **QUESTION 1: Show a data description.**\n",
    "1. Provide a summary of the data set to include descriptive statistics.\n",
    "2. Plot the time series.\n",
    "3. (optional) Provide any additional plots that may show important characteristics of your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce68dcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Preprocess Data\n",
    "sales = pd.read_csv('data/sales_data.csv')\n",
    "sales['date'] = pd.to_datetime(sales['date'], errors='coerce')\n",
    "sales['qty'] = pd.to_numeric(sales['qty'], errors='coerce')\n",
    "sales['val'] = pd.to_numeric(sales['val'], errors='coerce')\n",
    "# Subset and Group\n",
    "sales = sales[~sales['world'].isin(['gr', 'na'])]\n",
    "sales = sales.groupby('date').sum().reset_index().drop(columns=['world'])\n",
    "sales.sort_values('date', inplace=True)\n",
    "# Correct sales anaomaly\n",
    "sales.rename(columns={'qty' : 'order_quantity', 'val' : 'order_value'}, inplace=True)\n",
    "sales.loc[sales['order_quantity'].idxmax(), 'order_quantity'] = sales.loc[sales['order_quantity'].idxmax() - 1, 'order_quantity']\n",
    "sales.loc[sales['order_value'].idxmax(), 'order_value'] = sales.loc[sales['order_value'].idxmax() - 1, 'order_value']\n",
    "# Group by Week\n",
    "sales['date'] = sales['date'].dt.to_period('W').apply(lambda r: r.start_time)\n",
    "sales = sales.groupby('date').sum().reset_index()\n",
    "print(sales.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125ac0ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Stats and EDA\n",
    "print(\"Basic Descriptive Statistics:\")\n",
    "sales.describe().style.format(\"{:,.0f}\", subset=['order_quantity', 'order_value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6de6c17",
   "metadata": {},
   "source": [
    "The table above shows the descriptive stats of these data. For this assignment, I will be modeling order quantity. These data are weekly, so the date represents the first day of each week. The mean order quantity is ~2900 units, the max is ~16,000, and the minimum is 474. These data are highly variable with a standard deviation of ~2,000 units. Finally, we have plenty of data points (419 weeks) which is enough to detect patterns, trends, and seasonality in the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb4212c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Sales Qty Over Time\n",
    "plot = px.line(sales, x='date', y='order_quantity', \n",
    "               hover_data=['order_quantity'], \n",
    "               labels={'order_quantity':'Sales Quantity', \n",
    "                       'date':'Date'})\n",
    "plot.update_layout(title='Sales Over Time')\n",
    "plot.show()\n",
    "\n",
    "# Plot Sales Value\n",
    "plot = px.line(sales, x='date', y='order_value', \n",
    "               hover_data=['order_value'], \n",
    "               labels={'order_value':'Sales Value', \n",
    "                       'date':'Date'})\n",
    "plot.update_layout(title='Sales Value Over Time')\n",
    "plot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af85999",
   "metadata": {},
   "source": [
    "Based on these plots, there appears to be weak yearly seasonality, and a weak trend as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0159fa35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUCNTIONS\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "def diagnostic_plot(series, title=\"Time Series\"):\n",
    "    '''Plot Time Series, ACF, PACF, and Histogram for a given series.'''\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(2,2, figsize=(15,10))\n",
    "    fig.suptitle(f'Diagnostic Plots: {title}', fontsize=16, fontweight='bold')\n",
    "    # Time series plot\n",
    "    ax[0,0].plot(series, linewidth=1.5, color='darkblue')\n",
    "    ax[0,0].set_title('Original Time Series', fontweight='bold')\n",
    "    ax[0,0].set_xlabel('Time')\n",
    "    ax[0,0].set_ylabel('Value')\n",
    "    ax[0,0].grid(True, alpha=0.3)\n",
    "    # ACF plot\n",
    "    plot_acf(series, lags=min(60, len(series)//2), ax=ax[0,1], alpha=0.05)\n",
    "    ax[0,1].set_title('Autocorrelation Function (ACF)', fontweight='bold')\n",
    "    # PACF plot\n",
    "    plot_pacf(series, lags=min(60, len(series)//2), ax=ax[1, 0], \n",
    "              alpha=0.05, method='ywm')\n",
    "    ax[1, 0].set_title('Partial Autocorrelation Function (PACF)', fontweight='bold')\n",
    "    \n",
    "    # Histogram\n",
    "    ax[1, 1].hist(series, bins=30, density=True, alpha=0.7, \n",
    "                    color='darkblue', edgecolor='black')\n",
    "    ax[1, 1].set_title('Distribution', fontweight='bold')\n",
    "    ax[1, 1].set_xlabel('Value')\n",
    "    ax[1, 1].set_ylabel('Density')\n",
    "    ax[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "def adf_test(series):\n",
    "    '''Function to perform ADF test on series and print results.'''\n",
    "    result = adfuller(series)\n",
    "    print(\"ADF TEST RESULTS:\")\n",
    "    print('   ADF Statistic: {:.4f}'.format(result[0]))\n",
    "    print('   p-value: {:.4f}'.format(result[1]))\n",
    "    if result[1] < 0.05:\n",
    "        print(\"Conclusion: Reject Null Hypothesis -- Series is Stationary\")\n",
    "    else:\n",
    "        print(\"Conclusion: Fail to Reject Null Hypothesis -- Series is Non-Stationary\")\n",
    "def kpss_test(series):\n",
    "    '''function to perform KPSS test on series and print results...'''\n",
    "    result = kpss(series)\n",
    "    print(\"KPSS TEST RESULTS:\")\n",
    "    print('   KPSS Statistic: {:.4f}'.format(result[0]))\n",
    "    print('   p-value: {:.4f}'.format(result[1]))\n",
    "    if result[1] < 0.05:\n",
    "        print(\"   Conclusion: Reject Null Hypothesis -- Series is Non-Stationary\")\n",
    "    else:\n",
    "        print(\"   Conclusion: Fail to Reject Null Hypothesis -- Series is Stationary\")\n",
    "        \n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "def ljung_box_test(series, lags=10):\n",
    "    '''function to perform LB test on series and print results'''\n",
    "    result = acorr_ljungbox(series, lags=lags, return_df=True)\n",
    "    print(\"Ljung-Box Test Results: PVal < .05 means we reject the null and there IS autocorrelation in the data\")\n",
    "    return result.style.format(\"{:.5f}\", subset=['lb_stat', 'lb_pvalue'])\n",
    "\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "def stl_decomposition(series, series_name, period):\n",
    "    '''function to perform STL decompose and plot results'''\n",
    "    stl = STL(series, period=period)\n",
    "    result = stl.fit()\n",
    "    fig = result.plot()\n",
    "    fig.suptitle(f'STL Decomposition of {series_name}', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba419e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Diagnostic Plots -- SALES QUANTITY\n",
    "diagnostic_plot(sales['order_quantity'], title='Sales Quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2abe6a2",
   "metadata": {},
   "source": [
    "The autocorrelation plot slowly decreases, which is a sign of non stationarity. The PACF shows a significant spike at lag 1 and then quickly drops off, which suggests that AR(1) *could* be appropriate for this data. Before we do any modeling, however, I need to ensure that the data is stationary (I don't believe that it is given the ACF plot...)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4722839",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4a393",
   "metadata": {},
   "source": [
    "## **QUESTION 2**: (45 points): Apply Box Jenkins Methodology using ARIMA, SARIMA, or SARIMAX depending on your data to obtain and evaluate forecasts. \n",
    "1. *Identification*: Perform all the necessary steps for identification and include automatic and manual identification. Summarize the results from your identification process. \n",
    "2. *Estimation*: Perform all necessary steps for identification and summarize the results of your estimation process.\n",
    "3. *Diagnostic Checking*: Perform all necessary steps for diagnostic checking and summarize the results of your diagnostic checking process.\n",
    "4. *Forecasting*: Compare the performance of the forecasts of your selected models using the appropriate methods on a test data set. Show graphics of the forecast performance to include confidence intervals. Describe your results and conclusions about the usefulness of the models you evaluated to forecast in the application domain of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e096d2",
   "metadata": {},
   "source": [
    "### **Part A: Identification**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763a57eb",
   "metadata": {},
   "source": [
    "To identify the appropriate model(s) (and AR / MA orders), I will use ACF / PACF plots, the ADF test, the KPSS test, and the auto_arima function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d3a4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Diagnostics\n",
    "print(\"ACF / PACF Plots for Sales Quantity: (SAME AS ABOVE)\")\n",
    "diagnostic_plot(sales['order_quantity'], title='Sales Quantity')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ee04eb",
   "metadata": {},
   "source": [
    "Based on the ACF plot, there is a slow decay of autocorrelations at lags 1-60, which is a strong sign of non-stationarity. The PACF plot shows a significant spike at lag 1 and then quickly drops off; however, some of the lags after lag 1 are also significant. The spike at lag 1 suggests that AR(1) could be appropriate for the data. The data likely needs to be differenced, so I will assess the PACF and ACF plots after differencing to determine the appropriate AR and MA orders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "563a4c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf_test(sales['order_quantity']); print(\"\\n\")\n",
    "kpss_test(sales['order_quantity'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5d8b071",
   "metadata": {},
   "source": [
    "The ADF test and KPSS test conflict with each other. They both reject the null hypothesis. For ADF, this teams that the data are stationary, but for the KPSS test, this means that the data are nonstationary. The P-Value for the ADF test is close to .05, however, so it barely rejects the null hypothesis. Given the ACF plot, I do not believe these data to be stationary, so I will difference the data, and then reassess these diagnostics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3a809f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First Difference + Diagnostics\n",
    "sales['order_quantity_diff'] = sales['order_quantity'].diff()\n",
    "diagnostic_plot(sales['order_quantity_diff'].dropna(), title='First Difference of Sales Quantity')\n",
    "adf_test(sales['order_quantity_diff'].dropna()); print(\"\\n\")\n",
    "kpss_test(sales['order_quantity_diff'].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe05036",
   "metadata": {},
   "source": [
    "After taking the first difference, the ACF shows small negative spikes at lag 1-3 and then quickly drops off. This suggests that an MA(3) *could* be appropriate for this data. The PACF shows significant negative spikes at lags 1-4 and then it quickly drops off. This suggests that an AR(4) model *could* be appropriate for these data. Finally, the ADF and KPSS tests both conclude that the series IS stationary after this first difference, which corroborates what I see in the ACF and PACF plots. Lets see if auto_arima agrees with my conclusions..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48025412",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pmdarima as pm\n",
    "# First try a nonseasonal model\n",
    "print(\"Finding best arima based on AIC:\")\n",
    "auto_arima_model = pm.auto_arima(\n",
    "    sales['order_quantity'], \n",
    "    seasonal=False,\n",
    "    stepwise=True,\n",
    "    trace=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore', \n",
    "    information_criterion='aic'\n",
    ")\n",
    "print(auto_arima_model.summary())\n",
    "print(\"\\n\\nFinding best arima based on BIC:\")\n",
    "auto_arima_model = pm.auto_arima(\n",
    "    sales['order_quantity'], \n",
    "    seasonal=False,\n",
    "    stepwise=True,\n",
    "    trace=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore', \n",
    "    information_criterion='bic'\n",
    ")\n",
    "print(auto_arima_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01636f0e",
   "metadata": {},
   "source": [
    "The auto-arima model based on AIC suggests that the best model is ARIMA(4,1,3). This is what I observed in the manual diagnostics above as well! Based on BIC, the best ARIMA model is ARIMA(2,1,2). I will also try a seasonal ARIMA with the same AR and MA orders as the nonseasonal model to try to account for the weak yearly seasonality that I observed in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03c69ae8",
   "metadata": {},
   "source": [
    "### **PART B & C: ESTIMATION + DIAGNOSTIC CHECKING**\n",
    "\n",
    "Fit the model to the data.\n",
    "\n",
    "Perform all necessary steps for diagnostic checking and summarize the results of your diagnostic checking process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24418842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# STL Decomposition to Assess Components of Series...\n",
    "stl_decomposition(sales['order_quantity'], series_name='Sales Quantity', period=52)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b66d296",
   "metadata": {},
   "source": [
    "Based on the STL decomposition, there is a weak seasonal component. Based on this, I will try a seasonal ARIMA model as well as the nonseasonal ARIMA model below. Residual variance is also higher from weeks 250-300. I might be able to model this with an exogenous variable, as the heightened demand is a lagged result of the COVID pandemic (lagged because demand responded when inventory arrived). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1f7675",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Results DF\n",
    "model_results = {\n",
    "    'Model': [], \n",
    "    'MSE' : [],\n",
    "    'RMSE' : [],\n",
    "    'AIC' : [],\n",
    "    'BIC' : [], \n",
    "    'Heteroskedasticity P-Val': [], \n",
    "    'Ljung-Box Residual Autocorrelation P-Val' : []\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e313dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fit_plot(df, date_col, actual_col, fit_col, residuals, title):\n",
    "    # Diagnostic Plots... Actual vs Fitted, Residuals, ACF/PACF of Residuals, Histogram of Residuals\n",
    "    import scipy.stats as stats\n",
    "    plt.style.use('ggplot')\n",
    "    fig, ax = plt.subplots(3,2, figsize=(15,10))\n",
    "    fig.suptitle(f'Diagnostic Plots: {title}', fontsize=16, fontweight='bold')\n",
    "    # Actual vs Fitted\n",
    "    ax[0,0].plot(df[date_col], df[actual_col], label='Actual', color='blue')\n",
    "    ax[0,0].plot(df[date_col], df[fit_col], label='Fitted', color='orange')\n",
    "    ax[0,0].set_title('Actual vs Fitted Values', fontweight='bold')\n",
    "    ax[0,0].set_xlabel('Date')\n",
    "    ax[0,0].set_ylabel('Sales Quantity')\n",
    "    ax[0,0].legend()\n",
    "    ax[0,0].grid(True, alpha=0.3)\n",
    "    # Residuals\n",
    "    ax[0,1].plot(df[date_col], residuals, label='Residuals', color='red')\n",
    "    ax[0,1].set_title('Residuals Over Time', fontweight='bold')\n",
    "    ax[0,1].set_xlabel('Date')\n",
    "    ax[0,1].set_ylabel('Residual Value')\n",
    "    ax[0,1].legend()\n",
    "    ax[0,1].grid(True, alpha=0.3)\n",
    "    # ACF of Residuals\n",
    "    plot_acf(residuals, lags=min(60, len(residuals)//2), ax=ax[1, 0], alpha=0.05)\n",
    "    ax[1, 0].set_title('ACF of Residuals', fontweight='bold')\n",
    "    # PACF of Residuals\n",
    "    plot_pacf(residuals, lags=min(60, len(residuals)//2), ax=ax[1, 1], alpha=0.05, method='ywm')\n",
    "    ax[1, 1].set_title('PACF of Residuals', fontweight='bold')\n",
    "    # QQ plot of residuals\n",
    "    sm.qqplot(residuals, line='s', ax=ax[2, 0])\n",
    "    ax[2, 0].set_title('QQ Plot of Residuals', fontweight='bold')\n",
    "    # Histogram of Residuals\n",
    "    ax[2, 1].hist(residuals, bins=30, density=True, alpha=0.7, color='red', edgecolor='black')\n",
    "    mean = np.mean(residuals)\n",
    "    sigma = np.std(residuals)\n",
    "    x = np.linspace(mean - 4*sigma, mean + 4*sigma, 1000)\n",
    "    normal_pdf = stats.norm.pdf(x, mean, sigma)\n",
    "    ax[2, 1].plot(x, normal_pdf, color='blue', linestyle='--', label='Normal PDF')\n",
    "    ax[2, 1].legend()\n",
    "    ax[2, 1].set_title('Histogram of Residuals', fontweight='bold')\n",
    "    ax[2, 1].set_xlabel('Residual Value')\n",
    "    ax[2, 1].set_ylabel('Density')\n",
    "    ax[2, 1].grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def model_eval(fit_model):\n",
    "    # MSE\n",
    "    residuals = fit_model.resid\n",
    "    mse = np.mean(residuals**2)\n",
    "    # RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    # AIC and BIC\n",
    "    aic = fit_model.aic\n",
    "    bic = fit_model.bic\n",
    "    # Ljung Box Test on Residuals\n",
    "    ljung_box_results = acorr_ljungbox(residuals, lags=1, return_df=True)\n",
    "    ljung_box_pval = ljung_box_results['lb_pvalue'].iloc[0]\n",
    "    # Heteroskedasticity Test on Residuals\n",
    "    from statsmodels.stats.diagnostic import het_arch\n",
    "    arch_test = het_arch(residuals, nlags=12)\n",
    "    arch_pval = arch_test[1]\n",
    "    return mse, rmse, aic, bic, ljung_box_pval, arch_pval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59b66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "P = 4\n",
    "D = 1\n",
    "Q = 3\n",
    "Y = sales['order_quantity']\n",
    "\n",
    "# Fit Model and Output Summary\n",
    "model = ARIMA(Y, order=(P,D,Q))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Add fitted values to sales df for diagnostics\n",
    "FIT_COL = f'ARIMA_{P,D,Q}'\n",
    "sales[FIT_COL] = model_fit.fittedvalues\n",
    "residuals = model_fit.resid\n",
    "\n",
    "# Model Diagnostic Results\n",
    "model_fit_plot(sales, date_col='date', actual_col='order_quantity', fit_col=FIT_COL, residuals=residuals, title=f'ARIMA({P},{D},{Q})')\n",
    "mse, rmse, aic, bic, ljung_box_pval, arch_pval = model_eval(model_fit)\n",
    "model_results['Model'].append(f'ARIMA({P},{D},{Q})')\n",
    "model_results['MSE'].append(mse)\n",
    "model_results['RMSE'].append(rmse)\n",
    "model_results['AIC'].append(aic)\n",
    "model_results['BIC'].append(bic)\n",
    "model_results['Ljung-Box Residual Autocorrelation P-Val'].append(ljung_box_pval)\n",
    "model_results['Heteroskedasticity P-Val'].append(arch_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e30c2",
   "metadata": {},
   "source": [
    "Overall, this model fit well aside from the slight heteroskedasticity that I see above in the residuals plot. The ACF and PACF of the residuals show no spikes, however, which is a good sign. The histogram shows that the residuals are approximately normally distributed (slightly tighter than normal), which is also a good sign. Finally, the QQ plot shows that the residuals are approximately normally distributed as well.\n",
    "\n",
    "Based on the heteroskedasticity, I am going to log transform the data and refit the model to see if that helps... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57e3ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log Transform Data and Refit Model\n",
    "sales['log_order_quantity'] = np.log(sales['order_quantity'])\n",
    "auto_arima_model_log = pm.auto_arima(\n",
    "    sales['log_order_quantity'],\n",
    "    seasonal=False,\n",
    "    stepwise=True,\n",
    "    trace=True,\n",
    "    suppress_warnings=True,\n",
    "    error_action='ignore',\n",
    "    information_criterion='aic'\n",
    ")\n",
    "print(auto_arima_model_log.summary())\n",
    "diagnostic_plot(sales['log_order_quantity'].diff().dropna(), title='Log of Sales Quantity (First Difference)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945bd58",
   "metadata": {},
   "source": [
    "Based on the autoarima above, the best model to try after a log transform is a ARIMA(3,1,1). I differenced the data once, and this corroborates with what I see in the diagnostic plots as well. There are significant ACF spikes at lag 1, which suggests MA(1) would work well, and significant spikes at PACF lag 1-3 suggest that AR(3) would work well (spikes at lag 4-5 are boarderline...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefffae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Model\n",
    "P = 3\n",
    "D = 1\n",
    "Q = 1\n",
    "Y = sales['log_order_quantity']\n",
    "\n",
    "# Fit Model and Output Summary\n",
    "model = ARIMA(Y, order=(P,D,Q))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Add fitted values to sales df for diagnostics\n",
    "FIT_COL = f'LOG_ARIMA_{P,D,Q}'\n",
    "sales[FIT_COL] = model_fit.fittedvalues\n",
    "residuals = model_fit.resid\n",
    "\n",
    "# Model Diagnostic Results\n",
    "model_fit_plot(sales, date_col='date', actual_col='log_order_quantity', fit_col=FIT_COL, residuals=residuals, title=f'LOG ARIMA({P},{D},{Q})')\n",
    "mse, rmse, aic, bic, ljung_box_pval, arch_pval = model_eval(model_fit)\n",
    "model_results['Model'].append(f'LOG ARIMA({P},{D},{Q})')\n",
    "model_results['MSE'].append(mse)\n",
    "model_results['RMSE'].append(rmse)\n",
    "model_results['AIC'].append(aic)\n",
    "model_results['BIC'].append(bic)\n",
    "model_results['Ljung-Box Residual Autocorrelation P-Val'].append(ljung_box_pval)\n",
    "model_results['Heteroskedasticity P-Val'].append(arch_pval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21816f6f",
   "metadata": {},
   "source": [
    "The log transform helped dramatically to reduce the heteroskedasticity and outliers in the residuals. The ACF and PACF of the residuals show no spikes and appear to be white noise, which is a good sign. The histogram is also normally distributed, and the QQ plot shows that the residuals are approximately normally distributed as well. Overall, this model fit is better than the non-log transformed model, so I will move forward with this version. \n",
    "\n",
    "I am unsure why the prediction is 0 at T=0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4359d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARIMA LOG MODEL\n",
    "P = 3\n",
    "D = 1\n",
    "Q = 1\n",
    "P_S = 1\n",
    "D_S = 0\n",
    "Q_S = 1\n",
    "SEASON = 52\n",
    "Y = sales['log_order_quantity']\n",
    "\n",
    "# Fit Model and Output Summary\n",
    "model = ARIMA(Y, order=(P,D,Q), seasonal_order=(P_S, D_S, Q_S, SEASON))\n",
    "model_fit = model.fit()\n",
    "print(model_fit.summary())\n",
    "\n",
    "# Add fitted values to sales df for diagnostics\n",
    "FIT_COL = f'LOG_SARIMA_{P,D,Q}x{P_S,D_S,Q_S,SEASON}'\n",
    "sales[FIT_COL] = model_fit.fittedvalues\n",
    "residuals = model_fit.resid\n",
    "\n",
    "# Model Diagnostic Results\n",
    "model_fit_plot(sales, date_col='date', actual_col='log_order_quantity', fit_col=FIT_COL, residuals=residuals, title=f'LOG SARIMA({P},{D},{Q}x{P_S},{D_S},{Q_S},{SEASON})')\n",
    "mse, rmse, aic, bic, ljung_box_pval, arch_pval = model_eval(model_fit)\n",
    "model_results['Model'].append(f'LOG SARIMA({P},{D},{Q}x{P_S},{D_S},{Q_S},{SEASON})')\n",
    "model_results['MSE'].append(mse)\n",
    "model_results['RMSE'].append(rmse)\n",
    "model_results['AIC'].append(aic)\n",
    "model_results['BIC'].append(bic)\n",
    "model_results['Ljung-Box Residual Autocorrelation P-Val'].append(ljung_box_pval)\n",
    "model_results['Heteroskedasticity P-Val'].append(arch_pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42bc00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Results\n",
    "model_results_df = pd.DataFrame(model_results)\n",
    "model_results_df.style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d59896",
   "metadata": {},
   "source": [
    "In all cases, the heteroskedasticity P Value is very low, which suggests that there is still some heteroskedasticity in the residuals. This may be due to the odd spike at T=0, of which I am unsure of the cause...\n",
    "\n",
    "Otherwise, the SARIMA model appears to have the best fit in terms of AIC, MSE, and RMSE. In all cases, the Ljung Box test fails to reject the null hypothesis, which suggests that there is NO autocorrelation in the residuals (they are just white noise). \n",
    "\n",
    "I will transform the predictions back to the orginal scale and plot them against the actuals to see how each tracks the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828a4ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Predictions vs Actual for Each Model\n",
    "sales.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5656781d",
   "metadata": {},
   "source": [
    "### **PART D: FORECASTING**\n",
    "\n",
    "Compare the performance of the forecasts of your selected models using the appropriate methods on a test data set. Show graphics of the forecast performance to include confidence intervals. Describe your results and conclusions about the usefulness of the models you evaluated to forecast in the application domain of the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc2cb39e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tseries2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
